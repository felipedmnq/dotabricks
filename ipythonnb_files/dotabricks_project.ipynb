{"cells":[{"cell_type":"code","source":["import requests\nfrom typing import Dict, Tuple\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.dataframe import DataFrame\nfrom datetime import datetime\nimport argparse\n\ngcs_bucket = \"gs://dotabricks-felipedmnq\" # datalake \ngcs_promatches_hist_blob = f\"{gcs_bucket}/raw/pro_matches_history\"\npro_matches_endpoint = \"https://api.opendota.com/api/proMatches\" # last 100 matches"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4751ce89-ac2f-44e4-b6be-2e2de9fdde00","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# dbutils.fs.mkdirs(f\"{gcs_bucket}/raw\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0ece44b-edf5-4db4-8b15-6ea611d64f1f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# repartition - agg partitions to one unique partition - useful because our file is not big.\n# coalesce works in the same way?\n\n\n## REPARTITION returns a DF with exactly N partitions - WIDE transformation - SHUFFLE THE DATA - EVENLY BALANCED partition sizes.\n# df.repartition(1).write.format(\"parquet\").mode(\"append\").save(gcs_promatches_blob)\n\n## COALESCE returns a DF with exactly N partitions - NARROW transformation - NO SHUFFLE - NOT ABLE TO INCREASE PARTITIONS\n# df.coalesce(1).write.format(\"parquet\").mode(\"append\").save(gcs_promatches_blob)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e0430cc-2e57-4622-97e7-13770485b9a8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_data(url: str, **kwargs) -> Dict[str, any]:\n    \"\"\"Update url if needed. Get data from api as url specifications\"\"\" \n    \n    if kwargs:\n        params = \"&\".join([f\"{k}={v}\" for k, v in kwargs.items()])\n        url += f\"?{params}\"\n    \n    response = requests.get(url)\n    return response.json()\n\ndef get_min_match_id(df: DataFrame) -> int: \n    \"\"\"return the oldest match from datalake files\"\"\"\n\n    min_match_id = df.agg(F.min(\"match_id\")).collect()[0][0]\n    return min_match_id\n\ndef get_max_date(df: DataFrame) -> str:\n    max_date = df.withColumn(\n        \"match_date\", F.from_unixtime(\"start_time\")\n    ).agg(F.date_add(F.max(F.col(\"match_date\")), -1)).collect()[0][0]\n    \n    return max_date\n\ndef get_min_date(df: DataFrame) -> str:\n    min_date = df.withColumn(\n        \"match_date\", F.from_unixtime(\"start_time\")\n    ).agg(F.date_add(F.min(F.col(\"match_date\")), -1)).collect()[0][0]\n    \n    return min_date\n\ndef get_current_data_from_datalake() -> DataFrame:\n    \"\"\"Get current data from parquet files in Cloud GCS bucket\"\"\"\n    \n    current_data_from_blob = spark.read.format(\"parquet\").load(gcs_promatches_hist_blob)\n    return current_data_from_blob\n    \ndef save_data_to_hist_blob(df: DataFrame) -> None:\n    \"\"\"Append data to datalake pro_matches_history file\"\"\"\n    \n    df.coalesce(1).write.format(\"parquet\").mode(\"append\").save(gcs_promatches_hist_blob)\n    \ndef get_and_save(min_match_id: int) -> Tuple[DataFrame, int]:\n    \"\"\"\"\"\"\n    \n    new_data = get_data(pro_matches_endpoint, less_than_match_id=min_match_id)\n    df_new_data = spark.createDataFrame(new_data)\n    min_match_id = get_min_match_id(df_new_data)\n    save_data_to_hist_blob(df_new_data)\n    \n    return df_new_data, min_match_id\n  \ndef get_history_pro_matches() -> Tuple[DataFrame, int]:\n    \"\"\"Get raw data from andpoint based on last match_id from current data.\"\"\"\n    \n    df = get_current_data_from_datalake()\n    min_match_id = get_min_match_id(df)\n#     data = get_data(pro_matches_endpoint, less_than_match_id=min_match_id)\n#     new_data = get_and_save(min_match_id)\n#     \n#     min_match_id = get_min_match_id(df_new_data)\n    \n    while min_match_id is not None:\n        print(min_match_id)\n        try:\n            _, min_match_id = get_and_save(min_match_id)\n        except Exception as e:\n            print(e)\n            break\n            \ndef get_new_pro_matches():\n    df_from_lake = get_current_data_from_datalake()\n    min_match_id = get_min_match_id(df_from_lake)\n    max_date = get_max_date(df_from_lake)\n    min_date = get_min_date(df_from_lake)\n    processing_date = datetime.now().date()\n    print(f\"{max_date} - {processing_date} - {min_date}\")\n    \n    while max_date < processing_date:\n        try:\n            df_new_pro_matches, min_match_id = get_and_save(min_match_id)\n            print(min_match_id)\n            print(f\"PROCESSING DATE: {processing_date}\\nMAX DATE: {max_date}\\nMIN_DATE: {min_date}\")\n        except Exception as e:\n            print(e)\n            break"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ffb83bda-8ec4-4c15-97af-a2b2a92f2b5e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--mode\", choices=[\"new\", \"history\"], default=\"new\")\n\n    args = vars(parser.parse_args(args=[]))\n\n    if args[\"mode\"] == \"new\":\n        get_new_pro_matches()\n    elif args[\"mode\"] == \"history\":\n        get_history_pro_matches()\n        \nif __name__ == \"__main__\":\n    main()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b882ef7d-0643-4dd4-9b68-0f9b0e097196","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"{'mode': 'new'}\n2022-12-17 - 2022-12-20 - 2022-07-03\n6644158035\nPROCESSING DATE: 2022-12-20\nMAX DATE: 2022-12-17\nMIN_DATE: 2022-07-03\n6641736871\nPROCESSING DATE: 2022-12-20\nMAX DATE: 2022-12-17\nMIN_DATE: 2022-07-03\n6639898297\nPROCESSING DATE: 2022-12-20\nMAX DATE: 2022-12-17\nMIN_DATE: 2022-07-03\n6638230212\nPROCESSING DATE: 2022-12-20\nMAX DATE: 2022-12-17\nMIN_DATE: 2022-07-03\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["{'mode': 'new'}\n2022-12-17 - 2022-12-20 - 2022-07-03\n6644158035\nPROCESSING DATE: 2022-12-20\nMAX DATE: 2022-12-17\nMIN_DATE: 2022-07-03\n6641736871\nPROCESSING DATE: 2022-12-20\nMAX DATE: 2022-12-17\nMIN_DATE: 2022-07-03\n6639898297\nPROCESSING DATE: 2022-12-20\nMAX DATE: 2022-12-17\nMIN_DATE: 2022-07-03\n6638230212\nPROCESSING DATE: 2022-12-20\nMAX DATE: 2022-12-17\nMIN_DATE: 2022-07-03\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d0bb1e3-78fa-4ec6-8248-bffacc1e0d95","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"{'mode': 'new'}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["{'mode': 'new'}\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"926e5c01-13d5-4aed-a38e-243397e8ef36","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dotabricks_project","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2820101149215985,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":2820101149215972}},"nbformat":4,"nbformat_minor":0}
